{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xtK8jzXyqSY"
      },
      "outputs": [],
      "source": [
        "!pip install docx\n",
        "!pip install python-docx\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install sentence_transformers\n",
        "import spacy\n",
        "import nltk\n",
        "spacy.cli.download(\"es_dep_news_trf\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "import re\n",
        "import string\n",
        "import es_dep_news_trf\n",
        "import torch\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nltk\n",
        "import nltk.translate.meteor_score as meteor_score\n",
        "from nltk.tag import pos_tag\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "from math import log\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "gTUD3KFDzKIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка текста из файла Word\n",
        "def read_docx(file_path, language):\n",
        "    doc = docx.Document(file_path)\n",
        "    paragraphs = []\n",
        "    words = []\n",
        "    sentences = []\n",
        "    proper_nouns = []\n",
        "    punctuation = set(string.punctuation)\n",
        "    stop_words = set(stopwords.words(language))\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        if paragraph.text.strip():\n",
        "            paragraphs.append(paragraph.text)\n",
        "            # Разбить параграфы на предложения и слова для подсчета\n",
        "            sentences.extend(re.split(r'[.!?]', paragraph.text))\n",
        "            words.extend(paragraph.text.split())\n",
        "\n",
        "    # Удалить пустые строки после разделения на предложения\n",
        "    sentences = list(filter(None, sentences))\n",
        "\n",
        "    # Определение и подсчет имен собственных\n",
        "    for sent in sentences:\n",
        "        tokens = word_tokenize(sent)\n",
        "        tagged = pos_tag(tokens)\n",
        "        proper_nouns.extend([word for word in tokens if word.lower() not in stop_words and word not in punctuation and (all(ch.isalpha() or ch.isspace() for ch in word) or word.isdigit())])\n",
        "\n",
        "    proper_noun_counts = Counter(proper_nouns)\n",
        "\n",
        "    return {\n",
        "        'text': ' '.join(paragraphs),\n",
        "        'paragraph_count': len(paragraphs),\n",
        "        'word_count': len(words),\n",
        "        'sentence_count': len(sentences),\n",
        "        'proper_nouns': proper_noun_counts,\n",
        "        'sentences': sentences\n",
        "    }\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SdMWCN37zMny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка текста\n",
        "def preprocess_text(text, language):\n",
        "    if language.lower() == 'russian':\n",
        "      nlp = spacy.load(\"ru_core_news_sm\")\n",
        "    elif language.lower() == 'spanish':\n",
        "      nlp = spacy.load(\"es_dep_news_trf\")\n",
        "    # nlp = es_dep_news_trf.load()\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text, language=language)\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    punctuation = set(string.punctuation)\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words and word not in punctuation and (all(ch.isalpha() or ch.isspace() for ch in word) or word.isdigit())]\n",
        "\n",
        "    # Лемматизация\n",
        "    # nlp = es_dep_news_trf.load()\n",
        "    tokens = [token.lemma_ for token in nlp(' '.join(tokens))]\n",
        "\n",
        "    # Возвращение предобработанного текста в виде строки\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "C1iGVUsuzO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens(text, language):\n",
        "    punctuation = set(string.punctuation)\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    words = word_tokenize(text.lower())  # Привести к нижнему регистру и токенизировать\n",
        "    filtered_words = [word for word in text if word.lower() not in stop_words and word not in punctuation and (all(ch.isalpha() or ch.isspace() for ch in word) or word.isdigit())]\n",
        "    return words"
      ],
      "metadata": {
        "id": "ornfw6sPzP-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bm25(text, tokenized_words, k1=1.5, b=0.75):\n",
        "    # Предобработка текста и токенизация\n",
        "    tokenized_text = tokenized_words\n",
        "\n",
        "    # Словарь частот термов в документе\n",
        "    term_freq = Counter(tokenized_text)\n",
        "\n",
        "    # Вычисляем среднюю длину документа\n",
        "    avgdl = sum(term_freq.values()) / len(term_freq)\n",
        "\n",
        "    # Инициализация переменной для хранения суммы BM25 для каждого терма в тексте\n",
        "    bm25_dict = {}\n",
        "\n",
        "    # Рассчитываем BM25 для каждого терма\n",
        "    for term, freq in term_freq.items():\n",
        "        idf = log((len(tokenized_text) - freq + 0.5) / (freq + 0.5) + 1)  # +1 для избежания деления на ноль\n",
        "        bm25 = idf * (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * (len(tokenized_text) / avgdl)))\n",
        "        bm25_dict[term] = bm25\n",
        "\n",
        "    # Сортируем по убыванию BM25\n",
        "    bm25_dict = dict(sorted(bm25_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    return bm25_dict\n",
        "\n",
        "#################################################################################### Пример использования\n",
        "\n",
        "\n",
        "# Вывод BM25 для каждого слова\n",
        "# for word, bm25 in bm25_result.items():\n",
        "#     print(f\"{word}: {round(bm25, 2)}\")"
      ],
      "metadata": {
        "id": "--79xPo4zdzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_compare_with_tokens(tokens1, tokens2, k=1.5, b=0.75):\n",
        "    # Подсчет частот слов в каждом документе\n",
        "    word_counts_doc1 = defaultdict(int)\n",
        "    word_counts_doc2 = defaultdict(int)\n",
        "    for word in tokens1:\n",
        "        word_counts_doc1[word] += 1\n",
        "    for word in tokens2:\n",
        "        word_counts_doc2[word] += 1\n",
        "\n",
        "    # Получение всех уникальных слов\n",
        "    all_words = set(word_counts_doc1.keys()) | set(word_counts_doc2.keys())\n",
        "\n",
        "    # Подсчет количества документов, содержащих каждое слово\n",
        "    doc_freq = defaultdict(int)\n",
        "    for word in all_words:\n",
        "        if word in word_counts_doc1:\n",
        "            doc_freq[word] += 1\n",
        "        if word in word_counts_doc2:\n",
        "            doc_freq[word] += 1\n",
        "\n",
        "    # Вычисление IDF для каждого слова\n",
        "    num_docs = 2  # У нас два документа\n",
        "    idf = {word: np.log((num_docs - doc_freq[word] + 0.5) / (doc_freq[word] + 0.5) + 1) for word in all_words}\n",
        "\n",
        "    # Подготовка данных для вычисления BM25\n",
        "    bm25_values = np.zeros((2, len(all_words)))\n",
        "\n",
        "    for i, word in enumerate(all_words):\n",
        "        # Подсчет вхождений слова в каждый документ\n",
        "        word_freq_doc1 = word_counts_doc1[word]\n",
        "        word_freq_doc2 = word_counts_doc2[word]\n",
        "\n",
        "        # Вычисление BM25 для каждого документа\n",
        "        for j, word_freq in enumerate([word_freq_doc1, word_freq_doc2]):\n",
        "            doc_length = len(tokens1) if j == 0 else len(tokens2)\n",
        "            bm25_values[j][i] = (idf[word] * word_freq * (k + 1)) / (word_freq + k * (1 - b + b * (doc_length / len(all_words))))\n",
        "\n",
        "    # Нормализация BM25 векторов\n",
        "    normalized_bm25_values = bm25_values / np.linalg.norm(bm25_values, axis=1, keepdims=True)\n",
        "\n",
        "    # Создание словарей BM25 для каждого документа\n",
        "    bm25_dicts = [{word: normalized_bm25_values[0][i] for i, word in enumerate(all_words)},\n",
        "                  {word: normalized_bm25_values[1][i] for i, word in enumerate(all_words)}]\n",
        "\n",
        "    # Вычисление косинусного расстояния между BM25 векторами документов\n",
        "    cosine_similarity_bm25 = np.dot(normalized_bm25_values[0], normalized_bm25_values[1])\n",
        "\n",
        "    return bm25_dicts, cosine_similarity_bm25\n",
        "\n",
        "############################################################################## Пример использования функции с токенами\n",
        "\n",
        "\n",
        "# Вывод BM25 для каждого слова в каждом документе на основе токенов\n",
        "# for idx, bm25_dict_tokens in enumerate(bm25_result_tokens):\n",
        "#     print(f\"BM25 for Document {idx+1}:\")\n",
        "#     for word, bm25 in bm25_dict_tokens.items():\n",
        "#         print(f\"{word}: {bm25}\")\n",
        "#     print()\n",
        "\n",
        "# # Вывод косинусного расстояния между BM25 векторами документов на основе токенов\n",
        "# print(f\"Cosine Similarity (BM25) with tokens: {bm25_similarity_tokens}\")\n"
      ],
      "metadata": {
        "id": "LIaF220HzqbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_texts(text1, text2):\n",
        "    # Загрузка модели\n",
        "    model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
        "\n",
        "    # Получение векторных представлений для обоих текстов\n",
        "    embeddings = model.encode([text1, text2], convert_to_tensor=True)\n",
        "\n",
        "    # Вычисление косинусного расстояния между векторами\n",
        "    similarity = 1 - cosine(embeddings[0], embeddings[1])\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Пример использования функции\n",
        "\n",
        "# similarity_score = compare_texts(text1, text2)\n",
        "# print(\"Семантическая схожесть текстов:\", similarity_score)\n"
      ],
      "metadata": {
        "id": "4Q1TL5c_GnEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_texts_with_meteor(tokenized1, tokenized2):\n",
        "    # Токенизация предобработанных текстов\n",
        "    # tokenized_text1 = word_tokenize(preprocessed_text1)\n",
        "    # tokenized_text2 = word_tokenize(preprocessed_text2)\n",
        "\n",
        "    # Расчет метрики METEOR\n",
        "    meteor_similarity = meteor_score.single_meteor_score(tokenized_text1, tokenized_text2)\n",
        "\n",
        "    return meteor_similarity\n",
        "\n",
        "# Пример использования функции\n",
        "# similarity_score, t = compare_texts_with_meteor(text1, text2)\n",
        "# print(\"METEOR similarity between the texts:\", similarity_score)"
      ],
      "metadata": {
        "id": "QNhZ73PzTUnK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}